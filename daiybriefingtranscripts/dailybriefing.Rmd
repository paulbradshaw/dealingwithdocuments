---
title: "Daily Briefing"
author: "Paul Bradshaw"
date: "15/05/2019"
output: html_document
---

# Analysing daily briefing transcripts

We want to find out what issues have been mentioned most often in daily briefings.

How often are questions being asked about particular subjects (e.g. lockdown)
Who is asking the questions?
What outlets are they from
What roles do they hold?
What is the 'sentiment' (pos/neg)?
How long is given to questions?
How often are follow up questions being offered?



## Import the transcripts


```{r}
library(rio)
```


```{r}
scrapedata <- rio::import("dailybriefingscrapeApr2728.csv")
```


```{r}
csvlist <- ["dailybriefingscrapeApr2728.csv"]
for (i in csvlist){
  #add to existing dataset
}
```

## Clean the data

```{r}
colnames(scrapedata) <- c("url","text")
scrapedata$len <- nchar(scrapedata$text)
#Which lines have the datestamp
scrapedata$date <- grepl("[A-Z][a-z]{2} [0-9]{1,2}, 2020",scrapedata$text)
#Remove lines about the transcript
scrapedata$follow <- grepl("Follow Rev Transcripts",scrapedata$text)
scrapedata$archive <- grepl("Our public transcript archive",scrapedata$text)
scrapedata$valid <- grepl("Offer valid for new",scrapedata$text)
#Remove the rows we don't want
scrapedata <- subset(scrapedata, scrapedata$date != TRUE)
scrapedata <- subset(scrapedata, scrapedata$follow != TRUE)
scrapedata <- subset(scrapedata, scrapedata$archive != TRUE)
scrapedata <- subset(scrapedata, scrapedata$valid != TRUE)
#Remove the columns
scrapedata$date <- NULL
scrapedata$follow <- NULL
scrapedata$archive <- NULL
scrapedata$valid <- NULL
```

Extract the dates: a URL looks like "https://www.rev.com/blog/transcripts/united-kingdom-coronavirus-briefing-transcript-april-28"

```{r extract dates}
dates <- strsplit(scrapedata$url,"transcript-")
#grab the second item in each item in that list
urldates <- sapply(dates, "[[", 2)
scrapedata$datestamp = urldates
rm(dates, urldates)
```

Extract the speakers

```{r}
names <- strsplit(scrapedata$text,"(")
#grab the second item in each item in that list
urldates <- sapply(dates, "[[", 2)
scrapedata$datestamp = urldates
rm(dates, urldates)

```


We have generated an export, and then import that into R below:

```{r read in csv}
transcripts <- read.csv("dailybriefingspeechonly.csv")
```

We then generate a list of all the speakers covered by that large dataset, and store that separately to compare:

```{r create table of usernames}
speakers <- data.frame(table(transcripts$Speaker))
```


## Which terms are most common?

We are expecting that words like 'lockdown' might occur particularly frequently. Let's find out which ones.

First, we need to export the column of keywords:

```{r export as txt}
#This is based on steps outlined in a [blog post by John Victor Anderson](http://johnvictoranderson.org/?p=115). 
write.csv(transcripts$P, 'transcriptsastext.txt')
```

Now we re-import that data as a character object using `scan`:

```{r import and clean}
ttext <- scan('transcriptsastext.txt', what="char", sep=",")
# We convert all text to lower case to prevent any case sensitive issues with counting
ttext <- tolower(ttext)
#Replace new line code with a space
ttext <- gsub('\n', ' ', ttext)
#Unescape HTML - first activate the htmltools package
library(htmltools)
#Then run the htmlEscape function
ttext <- htmltools::htmlEscape(ttext)
#This doesn't seem to work 100%
```

We now need to put this through a series of conversions before we can generate a table:

```{r split on spaces, convert to table}
#Split the text on every space
ttext.split <- strsplit(ttext, " ")
#Create a vector
ttext.vec <- unlist(ttext.split)
#Convert that to a table
ttext.table <- table(ttext.vec)
#remove the objects created that we no longer need
rm(ttext.split, ttext.vec)
```

That table is enough to create a CSV from:

```{r export and read back as CSV}
write.csv(ttext.table, 'ttexttable.csv')
#read it back in
tdata <- read.csv('ttexttable.csv')
summary(tdata)
#rename the columns
colnames(tdata) <- c('index', 'word', 'freq' )
summary(tdata)

```

### Removing stopwords

We could strip out stopwords from our data [using `tidytext`'s stop words](https://stackoverflow.com/questions/43441884/removing-stop-words-with-tidytext?rq=1).

```{r remove stopwords}
library(tidyverse)
#Install tidytext which is needed for get_stopwords() 
library(tidytext)
#Use anti_join with stopwords fetched using get_stopwords to remove those stopwords from tweetdata and put in new object
cleaned_tdata <- tdata %>%
  anti_join(get_stopwords())
```

Let's use a simpler approach with `gsub` to remove punctuation

```{r gsub to replace characters}
#Just makes it simpler to reuse code below
cleaned_tweetdata <- cleaned_tdata
#gsub is used to replace any comma with nothing ""
#the results are used to create a new column
cleaned_tweetdata$wordnopunc <- gsub(",","",cleaned_tweetdata$word)
#That new column is overwritten with each new clean
cleaned_tweetdata$wordnopunc <- gsub("-","",cleaned_tweetdata$wordnopunc)
cleaned_tweetdata$wordnopunc <- gsub("!","",cleaned_tweetdata$wordnopunc)
cleaned_tweetdata$wordnopunc <- gsub("'","",cleaned_tweetdata$wordnopunc)
cleaned_tweetdata$wordnopunc <- gsub('"',"",cleaned_tweetdata$wordnopunc)
#This has to be escaped or it replaces all characters
cleaned_tweetdata$wordnopunc <- gsub("\\.","",cleaned_tweetdata$wordnopunc)
cleaned_tweetdata$wordnopunc <- gsub("\\?","",cleaned_tweetdata$wordnopunc)
cleaned_tweetdata$word2 <- NULL

#Then replace
cleaned_tdata <- cleaned_tweetdata 
rm(cleaned_tweetdata)
```



### SQL query: most common words

We can bring the most frequent words to the top using `sqldf`:

```{r top 100 occurring words}
sqldf::sqldf('SELECT wordnopunc, freq 
             FROM cleaned_tdata
             ORDER BY freq DESC
             LIMIT 100')
```

And export that as a CSV:

```{r export frequency list as CSV}
wordfreq <- sqldf::sqldf('SELECT wordnopunc, freq 
             FROM cleaned_tdata
             ORDER BY freq DESC
             LIMIT 1000')

write.csv(wordfreq, "wordfreq.csv")
```

NHS seems to appear 3 times. Why is that?

## Exporting this R Markdown file as pure R

The following line converts this Rmd file into an R file, with all the narrative rendered as comments.

```{r}
#See https://www.garrickadenbuie.com/blog/convert-r-markdown-rmd-files-to-r-scripts/
knitr::purl("meptweets.Rmd", "meptweets.R", documentation = 2)
```

