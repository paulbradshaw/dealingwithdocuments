{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"topicModelling_DrWhoTweets.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyMASuT0Mw/kOu7GBhKk7FXh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"w2gZXIqdUB-4"},"source":["# Analysing Dr Who tweets\n","\n","During the 50th anniversary episode of *Dr Who* I scraped over 35,000 tweets. This notebook details some ways to analyse that in Python."]},{"cell_type":"code","metadata":{"id":"BEu6J6xLTKca"},"source":["#import pandas to read the CSV file\n","import pandas as pd\n","#and numpy to deal with maths\n","import numpy as np\n","\n","# plotting packages\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# model building package\n","import sklearn\n","\n","# package to clean text\n","import re"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JG7TCOKGTMsS"},"source":["#I've published the Google spreadsheet as a CSV - store the url...\n","tweeturl = \"https://docs.google.com/spreadsheets/d/e/2PACX-1vTIpXKoHJGy-vA1iX2nuLYLrwog4IAHeufTrUaB3iGdF6yABBgW6ng6puehVkuLDN2kJHbnYEJ1_p9s/pub?gid=1257121167&single=true&output=csv\"\n","#and then read the CSV at that url\n","tweets = pd.read_csv(tweeturl)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":469},"id":"zC1bKXZJTVB1","executionInfo":{"status":"ok","timestamp":1637869412987,"user_tz":0,"elapsed":10,"user":{"displayName":"Paul Bradshaw","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYQSCAPAX7cikAjthcRciH5mrSmVPNLmT_aEplrw=s64","userId":"01457212023850926122"}},"outputId":"e36a866c-82eb-41a0-9241-5da56e0047e3"},"source":["#show the first few rows\n","tweets.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id_str</th>\n","      <th>tweet_url</th>\n","      <th>created_at</th>\n","      <th>text</th>\n","      <th>lang</th>\n","      <th>REGEX</th>\n","      <th>DALEK</th>\n","      <th>retweet_count</th>\n","      <th>screen_name</th>\n","      <th>hashtags</th>\n","      <th>query</th>\n","      <th>url</th>\n","      <th>user_mention</th>\n","      <th>media</th>\n","      <th>in_reply_to_screen_name</th>\n","      <th>in_reply_to_status_id</th>\n","      <th>lat</th>\n","      <th>lng</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>4.043630e+17</td>\n","      <td>https://twitter.com/HoptonChris/status/4043628...</td>\n","      <td>2013-11-23 21:36:29+00:00</td>\n","      <td>Question: Will Peter Carpaldi be known as 12 o...</td>\n","      <td>en</td>\n","      <td>False</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>HoptonChris</td>\n","      <td>DrWho</td>\n","      <td>#drwho</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>4.043630e+17</td>\n","      <td>https://twitter.com/EasyStreetD/status/4043628...</td>\n","      <td>2013-11-23 21:36:28+00:00</td>\n","      <td>RT @DenverComicCon: Which Dr. are you? Quick p...</td>\n","      <td>en</td>\n","      <td>False</td>\n","      <td>NaN</td>\n","      <td>1</td>\n","      <td>EasyStreetD</td>\n","      <td>DrWho</td>\n","      <td>#drwho</td>\n","      <td>http://bbc.in/1el7qo6</td>\n","      <td>DenverComicCon</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>4.043630e+17</td>\n","      <td>https://twitter.com/EastonNicky/status/4043628...</td>\n","      <td>2013-11-23 21:36:27+00:00</td>\n","      <td>RT @huxley06: Help us doctors ..you are our on...</td>\n","      <td>en</td>\n","      <td>False</td>\n","      <td>NaN</td>\n","      <td>2</td>\n","      <td>EastonNicky</td>\n","      <td>drwho</td>\n","      <td>#drwho</td>\n","      <td>NaN</td>\n","      <td>huxley06</td>\n","      <td>https://pbs.twimg.com/media/BZySuHPCMAAmpSF.jpg</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4.043630e+17</td>\n","      <td>https://twitter.com/jonnybeardo/status/4043628...</td>\n","      <td>2013-11-23 21:36:27+00:00</td>\n","      <td>Got to love #drwho #DayoftheDoctor</td>\n","      <td>en</td>\n","      <td>False</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>jonnybeardo</td>\n","      <td>drwho</td>\n","      <td>#drwho</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4.043630e+17</td>\n","      <td>https://twitter.com/juliaargy/status/404362836...</td>\n","      <td>2013-11-23 21:36:27+00:00</td>\n","      <td>@rod_ster #yougotme #hadtobail #DrWho</td>\n","      <td>en</td>\n","      <td>False</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>juliaargy</td>\n","      <td>yougotme</td>\n","      <td>#drwho</td>\n","      <td>NaN</td>\n","      <td>rod_ster</td>\n","      <td>NaN</td>\n","      <td>rod_ster</td>\n","      <td>4.043470e+17</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         id_str                                          tweet_url  ... lat lng\n","0  4.043630e+17  https://twitter.com/HoptonChris/status/4043628...  ... NaN NaN\n","1  4.043630e+17  https://twitter.com/EasyStreetD/status/4043628...  ... NaN NaN\n","2  4.043630e+17  https://twitter.com/EastonNicky/status/4043628...  ... NaN NaN\n","3  4.043630e+17  https://twitter.com/jonnybeardo/status/4043628...  ... NaN NaN\n","4  4.043630e+17  https://twitter.com/juliaargy/status/404362836...  ... NaN NaN\n","\n","[5 rows x 18 columns]"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rfPkjtvaTf4K","executionInfo":{"status":"ok","timestamp":1634710270031,"user_tz":-60,"elapsed":253,"user":{"displayName":"Paul Bradshaw","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYQSCAPAX7cikAjthcRciH5mrSmVPNLmT_aEplrw=s64","userId":"01457212023850926122"}},"outputId":"f9b6109c-a843-4be1-a5c3-08d2d43e773e"},"source":["#check what types the columns are\n","tweets.dtypes"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["id_str                     float64\n","tweet_url                   object\n","created_at                  object\n","text                        object\n","lang                        object\n","retweet_count                int64\n","screen_name                 object\n","hashtags                    object\n","query                       object\n","url                         object\n","user_mention                object\n","media                       object\n","in_reply_to_screen_name     object\n","in_reply_to_status_id      float64\n","lat                        float64\n","lng                        float64\n","dtype: object"]},"metadata":{},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"kRdFsC71U05w"},"source":["## Check how many tweets are unique\n","\n","*The rest of this code is from [this tutorial](https://ourcodingclub.github.io/tutorials/topic-modelling-python/)...*\n","\n","The `.unique()` function can tell us how many are... unique."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hy94ShVRocX-","executionInfo":{"status":"ok","timestamp":1637869541390,"user_tz":0,"elapsed":214,"user":{"displayName":"Paul Bradshaw","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYQSCAPAX7cikAjthcRciH5mrSmVPNLmT_aEplrw=s64","userId":"01457212023850926122"}},"outputId":"f4a41a13-e849-4f2f-f917-4ae5699f6b6f"},"source":["#show how many tweets\n","tweets['text'].shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(38586,)"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wFvivzXAU8v0","executionInfo":{"status":"ok","timestamp":1637869505309,"user_tz":0,"elapsed":233,"user":{"displayName":"Paul Bradshaw","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYQSCAPAX7cikAjthcRciH5mrSmVPNLmT_aEplrw=s64","userId":"01457212023850926122"}},"outputId":"aad9cc92-388e-484a-f65f-7e9bebf98bd9"},"source":["#show how many are unique\n","tweets['text'].unique().shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(31571,)"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BTgw5H0PTrAc","executionInfo":{"status":"ok","timestamp":1637869627628,"user_tz":0,"elapsed":217,"user":{"displayName":"Paul Bradshaw","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYQSCAPAX7cikAjthcRciH5mrSmVPNLmT_aEplrw=s64","userId":"01457212023850926122"}},"outputId":"86ad90ae-0f8c-4649-a739-dbd910e211d3"},"source":["# make a new column to highlight retweets\n","tweets['is_retweet'] = tweets['text'].apply(lambda x: x[:2]=='RT')\n","tweets['is_retweet'].sum()  # number of retweets"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["10842"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"gRuVbQItXY48","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637869660258,"user_tz":0,"elapsed":220,"user":{"displayName":"Paul Bradshaw","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYQSCAPAX7cikAjthcRciH5mrSmVPNLmT_aEplrw=s64","userId":"01457212023850926122"}},"outputId":"5a70d9b4-ada7-44ff-b7cb-ac2491c57ea5"},"source":["# number of unique retweets\n","tweets.loc[tweets['is_retweet']].text.unique().size"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["4132"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"t7aIJlXUl3LJ"},"source":["## Enter NLTK\n","\n","From section 6 of the tutorial at https://ourcodingclub.github.io/tutorials/topic-modelling-python/\n","\n","We need the `nltk` library to do topic modelling. Below we import that as well as some specific tools from that such as RegexpTokenizer, [which is described like this:](https://www.kite.com/python/docs/nltk.tokenize.regexp)\n","\n","> \"A RegexpTokenizer splits a string into substrings using a regular expression.\"\n","\n","While `stopwords` is a simple list of words like 'the', 'to', etc. which we are likely to want to remove from our analysis because of their high frequency and low significance."]},{"cell_type":"code","metadata":{"id":"aU-e9osUl-88"},"source":["import nltk\n","from nltk.tokenize import RegexpTokenizer\n","from nltk.corpus import stopwords"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ofa91q5Epwu5","executionInfo":{"status":"ok","timestamp":1637869883460,"user_tz":0,"elapsed":231,"user":{"displayName":"Paul Bradshaw","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYQSCAPAX7cikAjthcRciH5mrSmVPNLmT_aEplrw=s64","userId":"01457212023850926122"}},"outputId":"a17eb683-5bc4-4687-a2f2-ab1925cc59cb"},"source":["nltk.download('stopwords')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"L79Ej9aq52UJ"},"source":["### Defining some functions to remove links and users\n","\n","Two functions are created below to remove more elements which are unlikely to be relevant to analysis: links and users."]},{"cell_type":"code","metadata":{"id":"AeasJf2QmfKo"},"source":["def remove_links(tweet):\n","    '''Takes a string and removes web links from it'''\n","    tweet = re.sub(r'http\\S+', '', tweet) # remove http links\n","    tweet = re.sub(r'bit.ly/\\S+', '', tweet) # rempve bitly links\n","    tweet = tweet.strip('[link]') # remove [links]\n","    return tweet\n","\n","def remove_users(tweet):\n","    '''Takes a string and removes retweet and @user information'''\n","    tweet = re.sub('(RT\\s@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet) # remove retweet\n","    tweet = re.sub('(@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet) # remove tweeted at\n","    return tweet"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9f-rat0C6DqR"},"source":["Those two functions are used inside another function, defined below, which cleans tweets and then creates a 'token list'."]},{"cell_type":"code","metadata":{"id":"PQ_JWrj4t7qu"},"source":["my_stopwords = nltk.corpus.stopwords.words('english')\n","word_rooter = nltk.stem.snowball.PorterStemmer(ignore_stopwords=False).stem\n","my_punctuation = '!\"$%&\\'()*+,-./:;<=>?[\\\\]^_`{|}~•@'\n","\n","# cleaning master function\n","def clean_tweet(tweet, bigrams=False):\n","    tweet = remove_users(tweet)\n","    tweet = remove_links(tweet)\n","    tweet = tweet.lower() # lower case\n","    tweet = re.sub('['+my_punctuation + ']+', ' ', tweet) # strip punctuation\n","    tweet = re.sub('\\s+', ' ', tweet) #remove double spacing\n","    tweet = re.sub('([0-9]+)', '', tweet) # remove numbers\n","    tweet_token_list = [word for word in tweet.split(' ')\n","                            if word not in my_stopwords] # remove stopwords\n","\n","    tweet_token_list = [word_rooter(word) if '#' not in word else word\n","                        for word in tweet_token_list] # apply word rooter\n","    if bigrams:\n","        tweet_token_list = tweet_token_list+[tweet_token_list[i]+'_'+tweet_token_list[i+1]\n","                                            for i in range(len(tweet_token_list)-1)]\n","    tweet = ' '.join(tweet_token_list)\n","    return tweet"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BAL_qNu86DhI"},"source":["We then use that function to add another column to our dataframe containing the cleaned version of each tweet."]},{"cell_type":"code","metadata":{"id":"tZ34CtJHpw-y"},"source":["#apply the function 'clean_tweet' to the 'text' column of the tweets dataframe\n","#and create a new column with the reults\n","tweets['clean_tweet'] = tweets['text'].apply(clean_tweet)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1pmWRYuYt3Hj","executionInfo":{"status":"ok","timestamp":1637870056038,"user_tz":0,"elapsed":20,"user":{"displayName":"Paul Bradshaw","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYQSCAPAX7cikAjthcRciH5mrSmVPNLmT_aEplrw=s64","userId":"01457212023850926122"}},"outputId":"1be6a7ac-261b-441c-88db-23c1a1de72a9"},"source":["#show the first 10\n","tweets['clean_tweet'][:10]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    question peter carpaldi known #drwho #drwhoth ...\n","1     dr quick person quiz bbc honor #drwho th anni...\n","2                          help us doctor hope #drwho \n","3                      got love #drwho #dayofthedoctor\n","4                          #yougotme #hadtobail #drwho\n","5     total brilliant best cinema crowd ever #drwho...\n","6                                     awesom dr #drwho\n","7                      great men forg fire epic #drwho\n","8                         ahhh #drwho amaz #savetheday\n","9                                          #drwho epic\n","Name: clean_tweet, dtype: object"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nr-HXWB-3ruZ","executionInfo":{"status":"ok","timestamp":1637870056038,"user_tz":0,"elapsed":13,"user":{"displayName":"Paul Bradshaw","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYQSCAPAX7cikAjthcRciH5mrSmVPNLmT_aEplrw=s64","userId":"01457212023850926122"}},"outputId":"4b0654cb-7daf-40c0-8143-9904131dcc3e"},"source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","# the vectorizer object will be used to transform text to vector form\n","vectorizer = CountVectorizer(max_df=0.9, min_df=25, token_pattern='\\w+|\\$[\\d\\.]+|\\S+')\n","\n","# apply transformation\n","tf = vectorizer.fit_transform(tweets['clean_tweet']).toarray()\n","\n","# tf_feature_names tells us what word each column in the matric represents\n","tf_feature_names = vectorizer.get_feature_names()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n","  warnings.warn(msg, category=FutureWarning)\n"]}]},{"cell_type":"code","metadata":{"id":"M0yyh5HzlRLZ"},"source":["from sklearn.decomposition import LatentDirichletAllocation\n","\n","number_of_topics = 10\n","\n","model = LatentDirichletAllocation(n_components=number_of_topics, random_state=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k74dJTAAkyZq","executionInfo":{"status":"ok","timestamp":1637870141600,"user_tz":0,"elapsed":70579,"user":{"displayName":"Paul Bradshaw","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYQSCAPAX7cikAjthcRciH5mrSmVPNLmT_aEplrw=s64","userId":"01457212023850926122"}},"outputId":"58409b97-fee6-4ce2-adc0-a194e52815e5"},"source":["model.fit(tf)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["LatentDirichletAllocation(random_state=0)"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","metadata":{"id":"A20uDahbqkeA"},"source":["def display_topics(model, feature_names, no_top_words):\n","    topic_dict = {}\n","    for topic_idx, topic in enumerate(model.components_):\n","        topic_dict[\"Topic %d words\" % (topic_idx)]= ['{}'.format(feature_names[i])\n","                        for i in topic.argsort()[:-no_top_words - 1:-1]]\n","        topic_dict[\"Topic %d weights\" % (topic_idx)]= ['{:.1f}'.format(topic[i])\n","                        for i in topic.argsort()[:-no_top_words - 1:-1]]\n","    return pd.DataFrame(topic_dict)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UAe_9Cqc6XL7"},"source":["## Show the 10 'topics' extracted\n","\n","Below we display a table of the 10 topics and the words that appear most in each. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":418},"id":"-jGyXEFWqoz4","executionInfo":{"status":"ok","timestamp":1637870142373,"user_tz":0,"elapsed":7,"user":{"displayName":"Paul Bradshaw","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYQSCAPAX7cikAjthcRciH5mrSmVPNLmT_aEplrw=s64","userId":"01457212023850926122"}},"outputId":"65f62e1a-58ad-46c5-c9aa-ae3e8b658b58"},"source":["no_top_words = 10\n","display_topics(model, tf_feature_names, no_top_words)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Topic 0 words</th>\n","      <th>Topic 0 weights</th>\n","      <th>Topic 1 words</th>\n","      <th>Topic 1 weights</th>\n","      <th>Topic 2 words</th>\n","      <th>Topic 2 weights</th>\n","      <th>Topic 3 words</th>\n","      <th>Topic 3 weights</th>\n","      <th>Topic 4 words</th>\n","      <th>Topic 4 weights</th>\n","      <th>Topic 5 words</th>\n","      <th>Topic 5 weights</th>\n","      <th>Topic 6 words</th>\n","      <th>Topic 6 weights</th>\n","      <th>Topic 7 words</th>\n","      <th>Topic 7 weights</th>\n","      <th>Topic 8 words</th>\n","      <th>Topic 8 weights</th>\n","      <th>Topic 9 words</th>\n","      <th>Topic 9 weights</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>â</td>\n","      <td>2245.3</td>\n","      <td>#doctorwho</td>\n","      <td>1185.8</td>\n","      <td>th</td>\n","      <td>3455.6</td>\n","      <td>tardi</td>\n","      <td>660.2</td>\n","      <td>watch</td>\n","      <td>1561.1</td>\n","      <td>googl</td>\n","      <td>1605.0</td>\n","      <td>time</td>\n","      <td>690.4</td>\n","      <td>dalek</td>\n","      <td>1242.5</td>\n","      <td>doctor</td>\n","      <td>4790.4</td>\n","      <td>time</td>\n","      <td>1747.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>de</td>\n","      <td>2129.1</td>\n","      <td>#savetheday</td>\n","      <td>770.3</td>\n","      <td>anniversari</td>\n","      <td>2377.1</td>\n","      <td>ever</td>\n","      <td>630.5</td>\n","      <td>love</td>\n","      <td>922.3</td>\n","      <td>today</td>\n","      <td>1218.4</td>\n","      <td>go</td>\n","      <td>682.9</td>\n","      <td>david</td>\n","      <td>780.9</td>\n","      <td>#doctorwho</td>\n","      <td>2904.4</td>\n","      <td>year</td>\n","      <td>1678.5</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>el</td>\n","      <td>1013.1</td>\n","      <td>amp</td>\n","      <td>537.0</td>\n","      <td>happi</td>\n","      <td>1218.4</td>\n","      <td>#tardis</td>\n","      <td>544.5</td>\n","      <td>good</td>\n","      <td>741.4</td>\n","      <td>baker</td>\n","      <td>905.8</td>\n","      <td>like</td>\n","      <td>649.8</td>\n","      <td>tennant</td>\n","      <td>750.5</td>\n","      <td>day</td>\n","      <td>2476.2</td>\n","      <td>celebr</td>\n","      <td>1110.9</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>la</td>\n","      <td>560.1</td>\n","      <td>#thedayofthedoctor</td>\n","      <td>418.7</td>\n","      <td>birthday</td>\n","      <td>643.8</td>\n","      <td>best</td>\n","      <td>491.2</td>\n","      <td>get</td>\n","      <td>724.9</td>\n","      <td>tom</td>\n","      <td>873.5</td>\n","      <td>look</td>\n","      <td>565.6</td>\n","      <td>dr</td>\n","      <td>739.7</td>\n","      <td>#savetheday</td>\n","      <td>2296.8</td>\n","      <td>special</td>\n","      <td>1060.9</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>que</td>\n","      <td>538.1</td>\n","      <td>#dayofthedoctor</td>\n","      <td>372.8</td>\n","      <td>tomorrow</td>\n","      <td>619.8</td>\n","      <td>one</td>\n","      <td>312.4</td>\n","      <td>dr</td>\n","      <td>589.9</td>\n","      <td>doodl</td>\n","      <td>862.4</td>\n","      <td>dr</td>\n","      <td>527.2</td>\n","      <td>year</td>\n","      <td>456.3</td>\n","      <td>#dayofthedoctor</td>\n","      <td>2088.7</td>\n","      <td>rt</td>\n","      <td>769.1</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>€“</td>\n","      <td>440.9</td>\n","      <td>make</td>\n","      <td>341.4</td>\n","      <td>doctor</td>\n","      <td>586.6</td>\n","      <td>theme</td>\n","      <td>305.1</td>\n","      <td>awesom</td>\n","      <td>507.5</td>\n","      <td>dr</td>\n","      <td>807.1</td>\n","      <td>€¦</td>\n","      <td>492.0</td>\n","      <td>bow</td>\n","      <td>455.1</td>\n","      <td>#doctorwhoth</td>\n","      <td>958.3</td>\n","      <td>space</td>\n","      <td>687.1</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>en</td>\n","      <td>428.1</td>\n","      <td>today</td>\n","      <td>326.4</td>\n","      <td>pm</td>\n","      <td>527.1</td>\n","      <td>ã</td>\n","      <td>190.2</td>\n","      <td>see</td>\n","      <td>463.9</td>\n","      <td>smith</td>\n","      <td>663.7</td>\n","      <td>back</td>\n","      <td>405.0</td>\n","      <td>mark</td>\n","      <td>447.3</td>\n","      <td>wait</td>\n","      <td>491.9</td>\n","      <td>anniversari</td>\n","      <td>687.1</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>doodl</td>\n","      <td>339.6</td>\n","      <td>us</td>\n","      <td>309.0</td>\n","      <td>#savetheday</td>\n","      <td>520.2</td>\n","      <td>photo</td>\n","      <td>159.4</td>\n","      <td>episod</td>\n","      <td>453.4</td>\n","      <td>matt</td>\n","      <td>636.3</td>\n","      <td>think</td>\n","      <td>374.0</td>\n","      <td>room</td>\n","      <td>443.1</td>\n","      <td>new</td>\n","      <td>484.5</td>\n","      <td>adventur</td>\n","      <td>638.1</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>un</td>\n","      <td>281.6</td>\n","      <td>fan</td>\n","      <td>296.4</td>\n","      <td>weekend</td>\n","      <td>498.3</td>\n","      <td>made</td>\n","      <td>153.6</td>\n","      <td>oh</td>\n","      <td>430.9</td>\n","      <td>game</td>\n","      <td>597.3</td>\n","      <td>know</td>\n","      <td>367.4</td>\n","      <td>stand</td>\n","      <td>431.1</td>\n","      <td>bbc</td>\n","      <td>456.7</td>\n","      <td>#win</td>\n","      <td>558.1</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>hoy</td>\n","      <td>277.1</td>\n","      <td>€¦</td>\n","      <td>272.1</td>\n","      <td>dr</td>\n","      <td>456.7</td>\n","      <td>box</td>\n","      <td>141.7</td>\n","      <td>tonight</td>\n","      <td>404.0</td>\n","      <td>#drwhoth</td>\n","      <td>537.8</td>\n","      <td>amp</td>\n","      <td>352.3</td>\n","      <td>recept</td>\n","      <td>398.1</td>\n","      <td>excit</td>\n","      <td>434.9</td>\n","      <td>enter</td>\n","      <td>557.1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  Topic 0 words Topic 0 weights  ... Topic 9 words Topic 9 weights\n","0             â          2245.3  ...          time          1747.0\n","1            de          2129.1  ...          year          1678.5\n","2            el          1013.1  ...        celebr          1110.9\n","3            la           560.1  ...       special          1060.9\n","4           que           538.1  ...            rt           769.1\n","5            €“           440.9  ...         space           687.1\n","6            en           428.1  ...   anniversari           687.1\n","7         doodl           339.6  ...      adventur           638.1\n","8            un           281.6  ...          #win           558.1\n","9           hoy           277.1  ...         enter           557.1\n","\n","[10 rows x 20 columns]"]},"metadata":{},"execution_count":26}]},{"cell_type":"markdown","metadata":{"id":"xyRCAZL5r3pE"},"source":["## Seeing the topics\n","\n","Look down each column and you can see how each topic shares certain features: topic 2 is all about the anniversary/birthday; topic 3 is all about the tardis/box; topic 5 is all about Tom Baker and Matt Smith."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WvK_nZM1sPuU","executionInfo":{"status":"ok","timestamp":1637870556708,"user_tz":0,"elapsed":221,"user":{"displayName":"Paul Bradshaw","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYQSCAPAX7cikAjthcRciH5mrSmVPNLmT_aEplrw=s64","userId":"01457212023850926122"}},"outputId":"be1e26ed-ff68-434e-a526-fc833474d1d2"},"source":["topictable = display_topics(model, tf_feature_names, no_top_words)\n","topictable['Topic 5 words']"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0       googl\n","1       today\n","2       baker\n","3         tom\n","4       doodl\n","5          dr\n","6       smith\n","7        matt\n","8        game\n","9    #drwhoth\n","Name: Topic 5 words, dtype: object"]},"metadata":{},"execution_count":28}]}]}